{"cells":[{"source":"# Imports\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains.llm import LLMChain\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.vectorstores import Chroma\nfrom langchain.embeddings.openai import OpenAIEmbeddings\n\n# Create the embedding function\nembedding_function = OpenAIEmbeddings()\n\n# Load the database from existing embeddings\ndb = Chroma(persist_directory=\"../chroma-data-langchain-docs\", embedding_function=embedding_function)\n\n# Create a prompt with 3 variables: `context`, `question` and `history`\nprompt = PromptTemplate(\n    template=\"\"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\n<context>\n{context}\n</context>\n\n<chat-history>\n{history}\n</chat-history>\n\nQuestion: {question}\nHelpful Answer:\"\"\",\n    input_variables=[\"context\", \"question\", \"history\"]\n)\n\n# Create an LLM with ChatOpenAI\nllm = ChatOpenAI(temperature=0)\n\n# Create a chain to reformulate the question based on the chat history\nquestion_generation_prompt = PromptTemplate(\n    template=\"\"\"Based on the following chat history, reformulate the Human's question:\n<chat-history>\n{history}\nHuman: {question}\n</chat-history>\nRelevant question:\"\"\",\n    input_variables=[\"history\", \"question\"]\n)\ngenerate_question_chain = LLMChain(llm=llm, prompt=question_generation_prompt)\n\n# Create the chain\nmemory = ConversationBufferMemory(input_key=\"question\")\nqa_chain = LLMChain(llm=llm, prompt=prompt, memory=memory)\n","metadata":{"executionCancelledAt":null,"executionTime":442,"lastExecutedAt":1686649868459,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Imports\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains.llm import LLMChain\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.vectorstores import Chroma\nfrom langchain.embeddings.openai import OpenAIEmbeddings\n\n# Create the embedding function\nembedding_function = OpenAIEmbeddings()\n\n# Load the database from existing embeddings\ndb = Chroma(persist_directory=\"../chroma-data-langchain-docs\", embedding_function=embedding_function)\n\n# Create a prompt with 3 variables: `context`, `question` and `history`\nprompt = PromptTemplate(\n    template=\"\"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\n<context>\n{context}\n</context>\n\n<chat-history>\n{history}\n</chat-history>\n\nQuestion: {question}\nHelpful Answer:\"\"\",\n    input_variables=[\"context\", \"question\", \"history\"]\n)\n\n# Create an LLM with ChatOpenAI\nllm = ChatOpenAI(temperature=0)\n\n# Create a chain to reformulate the question based on the chat history\nquestion_generation_prompt = PromptTemplate(\n    template=\"\"\"Based on the following chat history, reformulate the Human's question:\n<chat-history>\n{history}\nHuman: {question}\n</chat-history>\nRelevant question:\"\"\",\n    input_variables=[\"history\", \"question\"]\n)\ngenerate_question_chain = LLMChain(llm=llm, prompt=question_generation_prompt)\n\n# Create the chain\nmemory = ConversationBufferMemory(input_key=\"question\")\nqa_chain = LLMChain(llm=llm, prompt=prompt, memory=memory)\n"},"cell_type":"code","id":"2167e559-9bf8-410c-b39e-b1a6f45fa3a2","execution_count":18,"outputs":[]},{"source":"# A function to chat with the bot.\n# This function will first rewrite the question so the chat history is taken into account\n# Then it will search for relevant documents based on the reformulated question\n# And finally call the qa_chain with the found context documents\ndef send_message(question):\n\n    # Reformulate the question based on history\n    new_question = generate_question_chain({\n        \"question\": question, \n        \"history\": memory.load_memory_variables({})[\"history\"]\n    })\n    print('Reformulated question:', new_question[\"text\"], '\\n')\n\n    # Query the database as store the results as `context_docs`\n    context_docs = db.similarity_search(new_question[\"text\"])\n\n    # Call the chain\n    result = qa_chain({\"context\": \"\\n\".join([d.page_content for d in context_docs]), \"question\": question})\n\n    # Print the result\n    print(result[\"text\"])","metadata":{"executionCancelledAt":null,"executionTime":11,"lastExecutedAt":1686649944082,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"\ndef send_message(question):\n\n    # Reformulate the question based on history\n    new_question = generate_question_chain({\n        \"question\": question, \n        \"history\": memory.load_memory_variables({})[\"history\"]\n    })\n    print('Reformulated question:', new_question[\"text\"], '\\n')\n\n    # Query the database as store the results as `context_docs`\n    context_docs = db.similarity_search(new_question[\"text\"])\n\n    # Call the chain\n    result = qa_chain({\"context\": \"\\n\".join([d.page_content for d in context_docs]), \"question\": question})\n\n    # Print the result\n    print(result[\"text\"])"},"cell_type":"code","id":"2791a89c-1178-47dd-b8ca-2f4ddc39e04f","execution_count":25,"outputs":[]},{"source":"send_message('Hi, my name is Emmanuel')","metadata":{"executionCancelledAt":null,"executionTime":1486,"lastExecutedAt":1686649947083,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"send_message('Hi, my name is Emmanuel')"},"cell_type":"code","id":"b8ca574d-c1cd-4052-b54b-66e8c4c33a81","execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":"Reformulated question: Can you assist me with something? \n\nHello Emmanuel, how can I assist you?\n"}]},{"source":"send_message('what is your name?')","metadata":{"executionCancelledAt":null,"executionTime":2388,"lastExecutedAt":1686649897557,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"send_message('what is your name?')"},"cell_type":"code","id":"f873bec2-561f-497e-b041-992e180e709e","execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":"Reformulated question: May I know your name?\nI am an AI language model, I do not have a name.\n"}]},{"source":"send_message('what is my name ?')","metadata":{"executionCancelledAt":null,"executionTime":2107,"lastExecutedAt":1686649908987,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"send_message('what is my name ?')"},"cell_type":"code","id":"ed514dd2-b776-4ab5-9e73-71705fa3c143","execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":"Reformulated question: Can you tell me what my name is?\nYour name is Emmanuel.\n"}]}],"metadata":{"language_info":{"name":"python","version":"3.8.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"editor":"DataCamp Workspace"},"nbformat":4,"nbformat_minor":5}