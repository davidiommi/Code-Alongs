{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcd52f5b-d79b-4e61-a44d-aaa20e720380",
   "metadata": {},
   "source": [
    "# Live training | 2023-06-13 | Building AI Applications with LangChain and GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42874dfa-76fc-4622-b2ab-1bbdd6c7f4f0",
   "metadata": {},
   "source": [
    "You've probably talked to ChatGPT using the web interface, or used the API with the `openai` python package and wondered \"what if I could teach it about my own data?\". Today we're going to build such an application using LangChain, a framework for developing applications powered by language models.\n",
    "\n",
    "In today's session, we'll build a chatbot powered by GPT-3.5 that can answer questions about LangChain, as it will have knowledge of the LangChain documentation. We'll cover:\n",
    "- Getting setup with an OpenAI developer account and integration with Workspace;\n",
    "- Install the LangChain package\n",
    "- Preparing the data\n",
    "- Embed the data using OpenAI's Embed API, and get a cost estimate for this operation\n",
    "- Storing the data in a vector database\n",
    "- How to query the vector database\n",
    "- Putting together a basic chat application to \"talk to the LangChain docs\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41be986-bad8-497c-80f8-11b50a7273f7",
   "metadata": {},
   "source": [
    "## Before you begin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a1012a-683a-40d0-8a36-da6df3b23154",
   "metadata": {},
   "source": [
    "### Create a developer account with OpenAI\n",
    "\n",
    "1. Go to the [API signup page](https://platform.openai.com/signup). \n",
    "\n",
    "2. Create your account (you'll need to provide your email address and your phone number).\n",
    "\n",
    "<img src=\"images/openai-create-account.jpeg\" width=\"200\">\n",
    "\n",
    "3. Go to the [API keys page](https://platform.openai.com/account/api-keys). \n",
    "\n",
    "4. Create a new secret key.\n",
    "\n",
    "<img src=\"images/openai-new-secret-key.png\" width=\"200\">\n",
    "\n",
    "5. **Take a copy of it**. (If you lose it, delete the key and create a new one.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee72ba1-5455-44df-9931-7bb7d4e407bf",
   "metadata": {},
   "source": [
    "### Add a payment method\n",
    "\n",
    "OpenAI sometimes provides free credits for the API, but it's not clear if that is worldwide or what the conditions are. You may need to add debit/credit card details. \n",
    "\n",
    "We will use 2 APIs:\n",
    "- The Chat API with the `gpt-3.5-turbo` model (cost: $0.002 / 1K tokens)\n",
    "- The Embedding API with the `Ada v2` model (cost: $0.0004 / 1K tokens)\n",
    "\n",
    "**In total, the Chat API (used for completions) should cost less than `$0.1` and embedding should cost around `$0.6`. This notebook provides embeddings already, so you can skip the embedding step.**\n",
    "\n",
    "1. Go to the [Payment Methods page](https://platform.openai.com/account/billing/payment-methods).\n",
    "\n",
    "2. Click Add payment method.\n",
    "\n",
    "<img src=\"images/openai-add-payment-method.png\" width=\"200\">\n",
    "\n",
    "3. Fill in your card details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415516c6-0c44-4409-9e7e-e683ab1cfbb2",
   "metadata": {},
   "source": [
    "### Set up Environment Variables\n",
    "\n",
    "1. In Workspace, click on Environment.\n",
    "\n",
    "<img src=\"images/workspace-environment.png\" width=\"200\">\n",
    "\n",
    "2. Click on the \"Environment Variables\" plus button.\n",
    "\n",
    "<img src=\"images/workspace-new-env-vars.png\" width=\"200\">\n",
    "\n",
    "3. In the \"Name\" field, type `OPENAI_API_KEY`. In the \"Value\" field, paste in your secret key (starting with `sk-`)\n",
    "\n",
    "<img src=\"images/workspace-create-env-vars.png\" width=\"300\">\n",
    "\n",
    "4. Click \"Create\", and connect the new integration.\n",
    "\n",
    "<img src=\"images/workspace-connect-env-vars.png\" width=\"300\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb20b7a5-5459-440d-af85-bc83704873ac",
   "metadata": {},
   "source": [
    "## Task 0: Setup\n",
    "\n",
    "For the purpose of this training, we'll need to install a few packages:\n",
    "- [`langchain`](https://python.langchain.com/en/latest/index.html): The LangChain framework, has many releases and must use a stable one\n",
    "- [`chromadb`](https://docs.trychroma.com/): The package we'll use for the vector database\n",
    "- [`tiktoken`](https://github.com/openai/tiktoken): A tokenizer we'll use to count GPT-3 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c29e2ef0-c6f2-4ce2-9c02-6c00f9cd155a",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 65917,
    "lastExecutedAt": 1709640232753,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# install openai (version 0.27.1)\n!pip install openai==0.27.1\n# install langchain (version 0.0.191)\n!pip install langchain==0.0.191\n# install chromadb\n!pip install chromadb==0.3.26\n# install tiktoken\n!pip install tiktoken==0.4.0",
    "outputsMetadata": {
     "0": {
      "height": 616,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# install openai (version 0.27.1)\n",
    "!pip install openai==0.27.1\n",
    "# install langchain (version 0.0.191)\n",
    "!pip install langchain==0.0.191\n",
    "# install chromadb\n",
    "!pip install chromadb==0.3.26\n",
    "# install tiktoken\n",
    "!pip install tiktoken==0.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46576543-af99-4a33-8dd1-fc01d5e476e4",
   "metadata": {},
   "source": [
    "## Task 1: Load data\n",
    "\n",
    "To be able to embed and store data, we need to provide LangChain with Documents. This is easy to achieve in LangChain thanks to [Document Loaders](https://python.langchain.com/en/latest/modules/indexes/document_loaders.html). In our case, we're targeting a \"Read the docs\" documentation, for which there is a loader [`ReadTheDocsLoader`](https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/readthedocs_documentation.html).\n",
    "In the folder `rtdocs`, you'll find all the HTML files from the LangChain documentation (https://python.langchain.com/en/latest/index.html). \n",
    "\n",
    "<details>\n",
    "    <summary>How did we obtain the data</summary>\n",
    "<br/><p>These file were downloaded by executing this linux command:</p>\n",
    "<pre>\n",
    "wget -r -A.html -P rtdocs https://python.langchain.com/en/latest/\n",
    "</pre>\n",
    "<p>We urge you **NOT** to execute this during the live training, as it will scan and download the full langchain doc site (~1000 files). This operation may be heavy and could disrupt the site, especially if hundreds of learners do it all at once!</p>\n",
    "</details>\n",
    "<br/><br/>\n",
    "\n",
    "\n",
    "Our first task is to load these HTML files as documents that we can use with langchain: we're going to use the `ReadTheDocsLoader`. It will read the directory containing all HTML files and transform them into `Document` objects. `ReadTheDocsLoader` will read each HTML file, remove HTML tags to only keep the text and return it as a `Document`. At the end of this task, we'll have a variable `raw_documents` containing a list of `Document`: one `Document` per HTML file. \n",
    "\n",
    "Note that in this step we won't actually load the documents into a database, we're simply loading the documents in a list.\n",
    "\n",
    "### Instructions\n",
    "1. import `ReadTheDocsLoader` from `langchain.document_loaders`\n",
    "2. Create the loader, pointing to the `rtdocs/python.langchain.com/en/latest` directory and enabling the HTML parser feature with `features='html.parser'`\n",
    "3. Load the data in `raw_documents` by calling `loader.load()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92e2087c-58cf-463f-9055-144a58d674a2",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 1131,
    "lastExecutedAt": 1709640245940,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Import ReadTheDocsLoader\nfrom langchain.document_loaders import ReadTheDocsLoader\n\n# Create a loader for the `rtdocs/python.langchain.com/en/latest` folder\nloader = ReadTheDocsLoader(\"rtdocs/python.langchain.com/en/latest\", features='html.parser')\n\n# Load the data\nraw_documents = loader.load()"
   },
   "outputs": [],
   "source": [
    "# Import ReadTheDocsLoader\n",
    "from langchain.document_loaders import ReadTheDocsLoader\n",
    "\n",
    "# Create a loader for the `rtdocs/python.langchain.com/en/latest` folder\n",
    "loader = ReadTheDocsLoader(\"rtdocs/python.langchain.com/en/latest\", features='html.parser')\n",
    "\n",
    "# Load the data\n",
    "raw_documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c09718-cddd-414b-a512-8c95d1cec69d",
   "metadata": {},
   "source": [
    "## Task 2: Slice the documents into smaller chunks\n",
    "\n",
    "In the previous step, we turned each HTML file into a Document. These files may be very long, and are potentially too large to embed fully. It's also a good practice to avoid embedding large documents:\n",
    "- long documents often contain several concepts. Retrieval will be easier if each concept is indexed separately;\n",
    "- retrieved documents will be injected in a prompt, so keeping them short will keep the prompt small(ish)\n",
    "\n",
    "LangChain has a collection of tools to do this: [Text Splitters](https://python.langchain.com/en/latest/modules/indexes/text_splitters.html). In our case, we'll be using the most straightfoward one and simplest to use: the [Recursive Character Text Splitter](https://python.langchain.com/en/latest/modules/indexes/text_splitters/examples/recursive_text_splitter.html). The recursive text splitter will recursively reduce the input by splitting it by paragraph, then sentences, then words as needed until the chunk is small enough. \n",
    "\n",
    "### Instructions\n",
    "1. Import the `RecursiveCharacterTextSplitter` from `langchain.text_splitter`\n",
    "2. Create a text splitter configured with `chunk_size=1000` and `chunk_overlap=200`  \n",
    "   _These values are arbitrary and you'll need to try different ones to see which best serve your use case_\n",
    "3. split the `raw_documents` and store them as `documents`, using the `.split_documents()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd571f4b-f8e6-474d-a837-7f0101f66c6f",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 11,
    "lastExecutedAt": 1709640248574,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Import RecursiveCharacterTextSplitter\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\n# Create the text splitter\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,\n    chunk_overlap=200,\n)\n\n# Split the documents\ndocuments = text_splitter.split_documents(raw_documents)"
   },
   "outputs": [],
   "source": [
    "# Import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Create the text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # arbitrary, experience with your own data is important\n",
    "    chunk_overlap=200, # choose as well, we have overlap to retain context\n",
    ")\n",
    "\n",
    "# Split the documents\n",
    "documents = text_splitter.split_documents(raw_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87b327b-a0cd-4e4a-8e59-5849317c4f22",
   "metadata": {},
   "source": [
    "## Task 3: count tokens and get a cost estimate of embedding\n",
    "\n",
    "We're now ready to embed our documents. Before we do so, we'd like to get an idea of how big it is and how much it will cost to embed. To do so, we'll use the [`tiktoken`](https://github.com/openai/tiktoken) library (no relation to TikTok, there is no dancing involved). tiktoken allows to encode and decode strings of text into tokens. In our case, we're mostly interested in how many tokens our documents translate to.\n",
    "\n",
    "> 💡 To better understand what a token is to GPT, head to [OpenAI's Tokenizer page](https://platform.openai.com/tokenizer) where you can see how a text translates to tokens.\n",
    "\n",
    "Prices for different models can be found on their [pricing page](https://openai.com/pricing).\n",
    "\n",
    "### Instructions\n",
    "1. Import `tiktoken`\n",
    "2. Create a tokenizer for the `text-embedding-ada-002` model using the `.encoding_for_model()` method\n",
    "3. Count tokens in each document using the `.encode()` method\n",
    "4. Calculate the sum of all tokens\n",
    "5. Calculate a cost estimate. The `text-embedding-ada-002` model costs `$0.0004` for 1000 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a331cba-51cc-4097-b077-ea91222858d9",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 1688,
    "lastExecutedAt": 1686229495256,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Import tiktoken\nimport tiktoken\n\n# Create an encoder \nencoder = tiktoken.encoding_for_model('text-embedding-ada-002')\n\n# Count tokens in each document\ndoc_tokens = [len(encoder.encode(d.page_content)) for d in documents]\n\n# Calculate the sum of all token counts\ntotal_tokens = sum(doc_tokens)\n\n# Calculate a cost estimate\ncost_estimate = (sum(doc_tokens)/1000) * 0.0004\nprint(f\"{total_tokens} tokens, cost estimate: ${cost_estimate:.2f}\")"
   },
   "outputs": [],
   "source": [
    "# Import tiktoken\n",
    "import tiktoken\n",
    "\n",
    "# Create an encoder \n",
    "encoder = tiktoken.encoding_for_model('text-embedding-ada-002')  # model used for the embedding\n",
    "\n",
    "# Count tokens in each document\n",
    "doc_tokens = [len(encoder.encode(d.page_content)) for d in documents]\n",
    "\n",
    "# Calculate the sum of all token counts\n",
    "total_tokens = sum(doc_tokens)\n",
    "\n",
    "# Calculate a cost estimate\n",
    "cost_estimate = (sum(doc_tokens)/1000) * 0.0004\n",
    "print(f\"{total_tokens} tokens, cost estimate: ${cost_estimate:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd4d3cf-f8ad-4acf-8eae-4a447bac06d4",
   "metadata": {},
   "source": [
    "## Task 4: embed the documents and store embeddings in the vector database\n",
    "\n",
    "We're now ready to embed our documents. Since embedding costs money, we'll want to save the embeddings into a database. LangChain can take care of all that using a [Vector Store](https://python.langchain.com/en/latest/modules/indexes/vectorstores.html).\n",
    "\n",
    "There are plenty of vector stores to choose from (see the [full list](https://python.langchain.com/en/latest/modules/indexes/vectorstores.html)). Today we'll use [Chroma](https://docs.trychroma.com/), but you could be using any other as they have the same interface in LangChain. Once again you'll need to try many of them to see which best fits your use case: some vector stores have specific features (like multimodality or multilingual), so be sure to check them out.\n",
    "\n",
    "Chroma is simple to use and can be persisted to disk. If you do not whish to embed the full set of documents yourself, feel free to skip this step and use the provided folder `chroma-data-langchain-docs`: we've already embedded all documents and persisted it in this folder.\n",
    "\n",
    "### Instructions\n",
    "1. Import `Chroma` from `langchain.vectorstores`\n",
    "2. Import `OpenAIEmbeddings` from `langchain.embeddings.openai` \n",
    "3. Create the embedding function\n",
    "4. Create a database from our documents, using `Chroma.from_documents()`. Pass the documents, embedding function and `persist_directory`.  \n",
    "   **Warning: executing this will embed thousands of documents and will cost about $0.6**\n",
    "5. Persist the data to disk by calling `.persist()` on the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc855f71-5844-4bc4-a6ce-4911e205c362",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 43240,
    "lastExecutedAt": 1686229544959,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Import chroma\nfrom langchain.vectorstores import Chroma\n\n# Import OpenAIEmbeddings\nfrom langchain.embeddings.openai import OpenAIEmbeddings\n\n# Create the mebedding function\nembedding = OpenAIEmbeddings()\n\n# Create a database from the documents and embedding function\ndb = Chroma.from_documents(documents=documents, embedding=embedding, persist_directory=\"chroma-data-langchain-docs\")\n\n# Persist the data to disk\ndb.persist()"
   },
   "outputs": [],
   "source": [
    "# Import chroma\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Import OpenAIEmbeddings\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "# Create the mebedding function\n",
    "embedding = OpenAIEmbeddings()\n",
    "\n",
    "# Create a database from the documents and embedding function\n",
    "#db = Chroma.from_documents(documents=documents, embedding=embedding, persist_directory=\"my-embeddings\")\n",
    "\n",
    "# Persist the data to disk\n",
    "#db.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d039e436-3f45-4625-adf3-6ffbd2b00442",
   "metadata": {},
   "source": [
    "## Alternative: use the provided embeddings\n",
    "\n",
    "We have already executed the step above to embed all documents and stored the result in the `chroma-data-langchain-docs` folder. Instead of embedding all the documents yourself, you can use these embeddings at no cost.\n",
    "\n",
    "The result of this step is the same as the step above, but will not call the OpenAI API and cost nothing.\n",
    "\n",
    "### Instructions\n",
    "1. Import `Chroma` from `langchain.vectorstores`\n",
    "2. Import `OpenAIEmbeddings` from `langchain.embeddings.openai`\n",
    "3. Create the embedding function\n",
    "4. Load the database from the `chroma-data-langchain-docs` directory, and provide the embedding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ab016d0-a2aa-43d4-8940-8881b53aabfa",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 2115,
    "lastExecutedAt": 1686658256058,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Import chroma\nfrom langchain.vectorstores import Chroma\n\n# Import OpenAIEmbeddings\nfrom langchain.embeddings.openai import OpenAIEmbeddings\n\n# Create the embedding function\nembedding = OpenAIEmbeddings()\n\n# Load the database from existing embeddings\ndb = Chroma(persist_directory=\"chroma-data-langchain-docs\", embedding_function=embedding)"
   },
   "outputs": [],
   "source": [
    "# Import chroma\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Import OpenAIEmbeddings\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "# Create the embedding function\n",
    "embedding = OpenAIEmbeddings()\n",
    "\n",
    "# Load the database from existing embeddings\n",
    "db = Chroma(persist_directory=\"chroma-data-langchain-docs\", embedding_function=embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea396f62-8ea0-49f3-9739-7d677b75a82d",
   "metadata": {},
   "source": [
    "## Step 5: query the vector database\n",
    "\n",
    "Now that we have a vector database, we can query it. A vector database stores embeddings (vectors) and allow to search through them using K-Nearest Neighbors algorithm (or a variation of it). When we query it the following will happen:\n",
    "1. Embed the text query to obtain a vector. It is crucial that this embedding is made using the same embedding technique that was used to embed the documents;\n",
    "2. Calculate the distance (or similarity) between the query vector and all other vectors;\n",
    "3. Sort results by similarity;\n",
    "4. Return the most similar documents.\n",
    "\n",
    "To do this with LangChain, we can use the `.similarity_search_with_score()` method of the database.\n",
    "\n",
    "### Instructions\n",
    "1. Call the `similarity_search_with_score` on `db` with the search query as parameter. Store the results in `results`;\n",
    "2. Print the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d510e945-335d-4db6-b3aa-75e5753373a9",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 695,
    "lastExecutedAt": 1686658277150,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Call the `similarity_search_with_score` method on `db`\nresults = db.similarity_search_with_score(\"how to load data from wikipedia?\")\n\n# Print the results\nfor (doc, score) in results:\n    print(f\"Score: {score}\\n{doc.page_content}\\n----\")"
   },
   "outputs": [],
   "source": [
    "# Call the `similarity_search_with_score` method on `db`\n",
    "results = db.similarity_search_with_score(\"how to load data from wikipedia?\")\n",
    "\n",
    "# Print the results\n",
    "for (doc, score) in results:\n",
    "    print(f\"Score: {score}\\n{doc.page_content}\\n----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca2c073-bff4-4d3b-be50-bf541a1d02b2",
   "metadata": {},
   "source": [
    "## Step 6: Create a QA chain\n",
    "\n",
    "Let's put it all together into a chat-like application. We want the user to ask a question, then search for relevant documents. We'll then create a prompt that includes the documents and the question so GPT can answer it (if possible).\n",
    "\n",
    "First, we'll query the database in a similar manner to previous step. We'll use `.similarity_search()`:\n",
    "\n",
    "```python\n",
    "question = \"show an example of adding memory to a chain\"\n",
    "context_docs = db.similarity_search(question)\n",
    "```\n",
    "\n",
    "Next, we will create a prompt that contains the question and the relevant documents:\n",
    "\n",
    "> You can think of a PromptTemplate as an fstring in python: values in curly brances are used as placeholder and will be replaced by values we pass when running the chain.\n",
    "\n",
    "```python\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\",\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "```\n",
    "\n",
    "To call the LLM with this prompt, we need to create an `LLMChain` and pass it an LLM and the prompt:\n",
    "\n",
    "```python\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "qa_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "```\n",
    "\n",
    "We can now call our chain like so:\n",
    "\n",
    "```python\n",
    "qa_chain({\"context\": \"<the context>\", \"question\": \"<the question>\"})\n",
    "```\n",
    "\n",
    "This will return a dict with a `text` key containing the LLM response.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. Import the necessary pieces:  \n",
    "   ```python\n",
    "   from langchain.prompts import PromptTemplate\n",
    "   from langchain.chains.llm import LLMChain\n",
    "   from langchain.chat_models import ChatOpenAI\n",
    "   ```\n",
    "2. Store the question as `question`, query the database and store the result as `context_docs`\n",
    "3. Create a prompt with 2 variables: `context` and `question`\n",
    "4. Create a chain with an LLM and the prompt\n",
    "5. Call the chain and print the result. The LLM output is in the `text` key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e8bd36b-ad02-4641-a188-ce4e957d68f9",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 3850,
    "lastExecutedAt": 1686646660242,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Import \nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains.llm import LLMChain\nfrom langchain.chat_models import ChatOpenAI\n\n# Set the question variable\nquestion = \"how can I make a chain remember previous messages?\"\n\n\n# Query the database as store the results as `context_docs`\ncontext_docs = db.similarity_search(question)\n\n# Create a prompt with 2 variables: `context` and `question`\nprompt = PromptTemplate(\n    template=\"\"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\n<context>\n{context}\n</context>\n\nQuestion: {question}\nHelpful Answer:\"\"\",\n    input_variables=[\"context\", \"question\"]\n)\n\n# Create an LLM with ChatOpenAI\nllm = ChatOpenAI(temperature=0)\n\n# Create the chain\nqa_chain = LLMChain(llm=llm, prompt=prompt)\n\n# Call the chain\nresult = qa_chain({\"context\": \"\\n\".join([d.page_content for d in context_docs]), \"question\": question})\n\n# Print the result\nprint(result[\"text\"])\n\n"
   },
   "outputs": [],
   "source": [
    "# Import \n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Set the question variable\n",
    "question = \"how can I make a chain remember previous messages?\"\n",
    "\n",
    "\n",
    "# Query the database as store the results as `context_docs`\n",
    "context_docs = db.similarity_search(question)\n",
    "\n",
    "# Create a prompt with 2 variables: `context` and `question`\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\",\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Create an LLM with ChatOpenAI\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Create the chain\n",
    "qa_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Call the chain\n",
    "result = qa_chain({\"context\": \"\\n\".join([d.page_content for d in context_docs]), \"question\": question})\n",
    "\n",
    "# Print the result\n",
    "print(result[\"text\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f633b2cc-dbdf-40f2-9561-a1227d8b8a7d",
   "metadata": {},
   "source": [
    "## To go further\n",
    "\n",
    "Our little chat app is working, but can be improved. Consider the following improvements:\n",
    "- **Clean up the documents**.  \n",
    "  Each document currently starts with useless text, and ends with a copyright notice. These texts do not provide value in our case and should be removed before embedding. Try to make an additional step after loading the `raw_documents`. In this step, iterate over all documents and find a way to remove unnecessary text.\n",
    "- **Add streaming to the LLM**.  \n",
    "  Instead of waiting for the full response, you can get a better experience by streaming the LLM response (much like the ChatGPT web interface). Look at the `playground.ipynb` notebook for an example, or check the [docs](https://python.langchain.com/en/latest/modules/models/chat/examples/streaming.html).\n",
    "- **Return sources to the user**.  \n",
    "  Getting a response is nice, but linking to the relevant docs is even better. There are a few techniques to return the source documents to the user. The simplest is to print the metadata of all the `context_docs` returned by the semantic search. In this example, you could use `doc.metadata[\"source\"]` to create a link to the langchain docs. A more advanced technique can be found [here](https://python.langchain.com/en/latest/modules/chains/index_examples/vector_db_qa_with_sources.html).\n",
    "- **Add memory support to the chain**.  \n",
    "  You can pass a `memory` argument to the `LLMChain`. Since we have multiple input variables, make sure to specify which one the memory should use: `ConversationBufferMemory(input_key=\"question\")`. You will also need to rerite the question before searching for documents, as the full context will not always be contained in the user input. You can find a working example in `going-further/adding-memory.ipynb`\n",
    "\n",
    "<br />\n",
    "\n",
    "### Useful links\n",
    "- Learn more about embeddings: https://txt.cohere.com/sentence-word-embeddings/\n",
    "- Learn more about vector databases: https://www.pinecone.io/learn/vector-database/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff39e9a-798f-42d9-8f71-3aaecb435a84",
   "metadata": {},
   "source": [
    "## LangChain caveats\n",
    "\n",
    "While LangChain is a great tool to discover concepts and techniques, it falls short to help you deliver a production-ready application:\n",
    "- Documentation is lacking. The documentation is not very complete and mostly made of examples, it lacks explanations and proper descriptions. To fully discover how a given module is working, you will need to peek in the code ([here on GitHub](https://github.com/hwchase17/langchain))\n",
    "- Too many abstractions. LangChain brings a lot of classes, some would argue too many. A `PromptTemplate` for example is pretty much the equivalent of a simple python f-string.\n",
    "- Very opinionated. As soon as you want to customize a chain (or agent), you will find that it's not always the smost simple thing. Many end-up re-implementing LangChain classes or functions to suit their needs better.\n",
    "\n",
    "LangChain is a great starting point and will let you write compelling demos very quickly. Consider these as prototypes, and not something that can be brought to production."
   ]
  }
 ],
 "metadata": {
  "editor": "DataCamp Workspace",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
